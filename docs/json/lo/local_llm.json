{"gems":{"name":"local_llm","downloads":57,"version":"0.1.1","version_created_at":"2025-12-02T06:19:18.632Z","version_downloads":20,"platform":"ruby","authors":"MD Abdul Barek","info":"local_llm is a lightweight Ruby gem that lets you interact with locally installed Ollama LLMs such as LLaMA, Mistral, CodeLLaMA, Qwen, and more. It supports configurable default models, configurable Ollama API endpoints, real-time streaming or non-streaming responses, and both one-shot and multi-turn chatâ€”while keeping all inference fully local, private, and offline.","licenses":["MIT"],"metadata":{"homepage_uri":"https://github.com/barek2k2/local_llm","changelog_uri":"https://github.com/barek2k2/local_llm/releases","bug_tracker_uri":"https://github.com/barek2k2/local_llm/issues","source_code_uri":"https://github.com/barek2k2/local_llm","allowed_push_host":"https://rubygems.org"},"yanked":false,"sha":"6520606fcdbc8558bf56986b84a87d4428d0413f56c2573bc8583f42f96d49f4","spec_sha":"ab85d5fc202812e96a0b3047261e33430f1b6102807ac6e3ef6f70224de3f1b1","project_uri":"https://rubygems.org/gems/local_llm","gem_uri":"https://rubygems.org/gems/local_llm-0.1.1.gem","homepage_uri":"https://github.com/barek2k2/local_llm","wiki_uri":null,"documentation_uri":null,"mailing_list_uri":null,"source_code_uri":"https://github.com/barek2k2/local_llm","bug_tracker_uri":"https://github.com/barek2k2/local_llm/issues","changelog_uri":"https://github.com/barek2k2/local_llm/releases","funding_uri":null,"dependencies":{"development":[],"runtime":[]}},"vcs_name":"GitHub","ci":null,"vcs_uri":"https://github.com/barek2k2/local_llm"}