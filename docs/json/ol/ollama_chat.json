{"gems":{"name":"ollama_chat","downloads":18026,"version":"0.0.70","version_created_at":"2026-02-23T19:51:32.064Z","version_downloads":12,"platform":"ruby","authors":"Florian Frank","info":"The app provides a command-line interface (CLI) to an Ollama AI model,\nallowing users to engage in text-based conversations and generate\nhuman-like responses. Users can import data from local files or web pages,\nwhich are then processed through three different modes: fully importing the\ncontent into the conversation context, summarizing the information for\nconcise reference, or storing it in an embedding vector database for later\nretrieval based on the conversation.\n","licenses":["MIT"],"metadata":{},"yanked":false,"sha":"af587650f9e20ed2e4fcb2f1f52f2343e2d661e6927240a5f79fcad3566c1215","spec_sha":"1673b7e0c54d22b9534d104402e739a4874b2b01a4ce04e49964e1d1d751559d","project_uri":"https://rubygems.org/gems/ollama_chat","gem_uri":"https://rubygems.org/gems/ollama_chat-0.0.70.gem","homepage_uri":"https://github.com/flori/ollama_chat","wiki_uri":null,"documentation_uri":"https://www.rubydoc.info/gems/ollama_chat/0.0.70","mailing_list_uri":null,"source_code_uri":null,"bug_tracker_uri":null,"changelog_uri":null,"funding_uri":null,"dependencies":{"development":[{"name":"all_images","requirements":"~> 0.12"},{"name":"context_spook","requirements":">= 0"},{"name":"debug","requirements":">= 0"},{"name":"gem_hadar","requirements":">= 2.17.0"},{"name":"kramdown","requirements":"~> 2.0"},{"name":"rspec","requirements":"~> 3.2"},{"name":"simplecov","requirements":">= 0"},{"name":"utils","requirements":">= 0"},{"name":"webmock","requirements":">= 0"}],"runtime":[{"name":"amatch","requirements":"~> 0.4"},{"name":"bigdecimal","requirements":"~> 3.1"},{"name":"complex_config","requirements":"~> 0.22, >= 0.22.2"},{"name":"const_conf","requirements":"~> 0.3"},{"name":"context_spook","requirements":"~> 1.5"},{"name":"csv","requirements":"~> 3.0"},{"name":"documentrix","requirements":">= 0.0.4"},{"name":"excon","requirements":"~> 1.0"},{"name":"infobar","requirements":">= 0.13.1"},{"name":"kramdown-ansi","requirements":"~> 0.3"},{"name":"mime-types","requirements":"~> 3.0"},{"name":"ollama-ruby","requirements":"~> 1.18"},{"name":"pdf-reader","requirements":"~> 2.0"},{"name":"redis","requirements":"~> 5.0"},{"name":"reverse_markdown","requirements":"~> 3.0"},{"name":"rss","requirements":"~> 0.3"},{"name":"rubyzip","requirements":"~> 3.0"},{"name":"search_ui","requirements":"~> 0.0"},{"name":"term-ansicolor","requirements":"~> 1.11"},{"name":"tins","requirements":"~> 1.52"},{"name":"unix_socks","requirements":"~> 0.3"}]}},"vcs_name":"GitHub","ci":1,"github_actions":1,"vcs_uri":"https://github.com/flori/ollama_chat"}